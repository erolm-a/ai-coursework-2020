{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POLICY SEARCH AGENT\n",
    "An AI agent that updates its policy model with each episode until it reaches optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use full window width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Imports\n",
    "'''\n",
    "#!pip install q tensorflow==1.15\n",
    "#!pip install q keras==2.2.4\n",
    "#!pip install utils\n",
    "\n",
    "import os\n",
    "os.chdir('..')\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import itertools\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "# Import the open AI gym\n",
    "import gym\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import utils as np_utils\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers.experimental import RandomFourierFeatures\n",
    "\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "# The basics\n",
    "%matplotlib inline\n",
    "import virl\n",
    "import utils\n",
    "\n",
    "# basic tools for defining the function and doing the gradient-based learning\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "# Policy stuff?\n",
    "import pandas as pd\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting stats function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a couple of helper functions to plot the results\n",
    "\n",
    "def plot_graphs(episode):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    # Fig 1: States\n",
    "    labels = ['s[0]: susceptibles', 's[1]: infectious', 's[2]: quarantined', 's[3]: recovereds']\n",
    "    states = np.array(episode.states)\n",
    "    #print(states.shape)\n",
    "    for i in range(4):\n",
    "        axes[0].plot(states[:,i], label=labels[i]);\n",
    "    axes[0].set_title('States')\n",
    "    axes[0].set_xlabel('weeks since start of epidemic')\n",
    "    axes[0].set_ylabel('State s(t)')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Fig 2: Actions\n",
    "    axes[1].plot(episode.actions);\n",
    "    axes[1].set_title('Action')\n",
    "    axes[1].set_xlabel('weeks since start of epidemic')\n",
    "    axes[1].set_ylabel('Action a(t)')\n",
    "    \n",
    "    # Fig 3: Rewards \n",
    "    axes[2].plot(episode.rewards);\n",
    "    axes[2].set_title('Reward, total: ' + str(sum(episode.rewards)))\n",
    "    axes[2].set_xlabel('weeks since start of epidemic')\n",
    "    axes[2].set_ylabel('reward r(t)')\n",
    "\n",
    "    #print('total reward', np.sum(rewards))\n",
    "        \n",
    "def plot_episode_stats(stats, smoothing_window=10, noshow=False):\n",
    "    # Plot the episode length over time\n",
    "    fig1 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(stats.episode_lengths)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Length\")\n",
    "    plt.title(\"Episode Length over Time\")\n",
    "    if noshow:\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show(fig1)\n",
    "\n",
    "    # Plot the episode reward over time\n",
    "    fig2 = plt.figure(figsize=(10,5))\n",
    "    rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "    plt.plot(rewards_smoothed)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Reward (Smoothed)\")\n",
    "    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n",
    "    if noshow:\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show(fig2)\n",
    "\n",
    "    # Plot time steps and episode number\n",
    "    fig3 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(np.cumsum(stats.episode_lengths), np.arange(len(stats.episode_lengths)))\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Episode\")\n",
    "    plt.title(\"Episode per time step\")\n",
    "    if noshow:\n",
    "        plt.close(fig3)\n",
    "    else:\n",
    "        plt.show(fig3)\n",
    "\n",
    "    return fig1, fig2, fig3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define couple of named tuples to hold useful episode information\n",
    "EpisodeStats: Information about episode length and total reward for episode\n",
    "EpisodeSteps: Information about the state entered, action taken and reward obtained in each step\n",
    "'''\n",
    "\n",
    "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
    "EpisodeSteps = namedtuple(\"Steps\", [\"states\", \"actions\", \"rewards\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SEED=42\n",
    "\n",
    "class PolicySearchAgent:\n",
    "    '''\n",
    "    An AI Agent that adjusts the policy function (which maps states to actions) to improve its performance\n",
    "    Achieves this by parameterizing the policy function which could allow it to be differentiable and updating the \n",
    "    parameters until the policy reaches optimization\n",
    "    '''\n",
    "    def __init__(self, env, alpha=1, gamma=1, rbf_type=\"gaussian\", out_dim_rbf=4096):\n",
    "        \"\"\"\n",
    "\n",
    "        :param alpha the learning rate\n",
    "        :param gamma discount rate\n",
    "        :param rbf_type can be \"gaussian\" or \"laplacian\". See RandomFourierFeatures\n",
    "        \"\"\"\n",
    "        # Neural network vars\n",
    "        self.dim_action = env.actions.shape\n",
    "        self.dim_observation = env.observation_space.shape\n",
    "        self.alpha = alpha\n",
    "        self.rbf_type = rbf_type\n",
    "        self.out_dim_rbf = out_dim_rbf\n",
    "        \n",
    "        self.n_states = 4 # take the state value directly\n",
    "        \n",
    "        # Update vars\n",
    "        self.gamma = gamma\n",
    "        self.episode = EpisodeSteps(states=[], actions=[], rewards=[])\n",
    "        self.steps = 0 \n",
    "        \n",
    "        # Build and train neural network\n",
    "        self.__build_model()\n",
    "        self.__build_train_fn()\n",
    "        \n",
    "    '''\n",
    "    Tabular methods implementation of build_model = constructing a neural network\n",
    "    Creates a stochastic policy representation that is differentiable, using softmax function\n",
    "    '''\n",
    "    def __build_model(self):\n",
    "        # print(\"build: \" + str(self.dim_observation))\n",
    "        # n_states = self.bins**4 # each state can occupy a bin, total n = n_bins[0] * n_bins[1] + ...\n",
    "        \n",
    "        self.model = keras.Sequential([\n",
    "          keras.Input(shape=(self.n_states,)),\n",
    "          RandomFourierFeatures(\n",
    "              output_dim=self.out_dim_rbf,\n",
    "              scale=10.,\n",
    "              kernel_initializer=self.rbf_type),\n",
    "          layers.Dense(units=self.n_states, activation='softmax',\n",
    "                       kernel_initializer=initializers.glorot_normal(seed=GLOBAL_SEED))\n",
    "        ])\n",
    "        \n",
    "    '''\n",
    "    Tabular methods implementation of build_fn = training the neural network\n",
    "    Custom train function that that replaces `model.fit(X, y)`because we use the output of model and use it for training\n",
    "    Called using self.train_fn([state, action_one_hot, target])` which would train the model\n",
    "    '''\n",
    "    def __build_train_fn(self):\n",
    "        # predefine a few variables\n",
    "        action_onehot_placeholder = K.placeholder(shape=(None, self.dim_action[0]),name=\"action_onehot\") # define a variable\n",
    "        target = K.placeholder(shape=(None,), name=\"target\") # define a variable       \n",
    "        \n",
    "        # this part defines the loss and is very important!\n",
    "        action_prob = self.model.output # the outpout of the neural network        \n",
    "        action_selected_prob = K.sum(action_prob * action_onehot_placeholder, axis=1) # probability of the selcted action        \n",
    "        log_action_prob = K.log(action_selected_prob) # take the log\n",
    "        loss = -log_action_prob * target # the loss we are trying to minimise\n",
    "        loss = K.mean(loss)\n",
    "        \n",
    "        # defining the speific optimizer to use\n",
    "        adam = optimizers.Adam(lr=self.alpha)# clipnorm=1000.0) # let's use a kereas optimiser called Adam\n",
    "        updates = adam.get_updates(params=self.model.trainable_weights, loss=loss) # what gradient updates to we parse to Adam\n",
    "            \n",
    "        \n",
    "        # create a handle to the optimiser function    \n",
    "        self.train_fn = K.function(inputs=[self.model.input, action_onehot_placeholder, target],\n",
    "                                   outputs=[],\n",
    "                                   updates=updates) # return a function which, when called, takes a gradient step\n",
    "\n",
    "        \n",
    "    '''\n",
    "    Tabular methods using discretized state space, digitizes a state into a set of bins\n",
    "    Encodes the state as one-hot\n",
    "    '''\n",
    "    def __get_state_encoding(self, state):\n",
    "        # Create a set of bins to put vallues into\n",
    "        #bins = np.linspace(0, self.population, self.bins+1)\n",
    "        return np.log(np.array(state) + 1)\n",
    "    \n",
    "    '''\n",
    "    Make prediction in current state of policy function, using the discretized state space for this task\n",
    "    ''' \n",
    "    def predict(self, state, action=None):\n",
    "        log_state = self.__get_state_encoding(state)\n",
    "        \n",
    "        if action is None:            \n",
    "            return self.model.predict(log_state).squeeze()\n",
    "        else:   \n",
    "            return self.model.predict(log_state)[action]\n",
    "    \n",
    "    '''\n",
    "    Incrementing the policy search object to update it with new values at each simulation step, once episode length is \n",
    "    reached process the data obtained during episode to update the parameters of the policy search function\n",
    "    '''\n",
    "    def increment(self, state, action, reward):\n",
    "        # During episode, increment steps and store latest state, action and reward values\n",
    "        self.steps += 1\n",
    "        self.episode.states.append(state)\n",
    "        self.episode.actions.append(action)\n",
    "        self.episode.rewards.append(reward)\n",
    "            \n",
    "    '''\n",
    "    Once an episode has ended, calculate the gradient of the policy function and retrain it with new parameters\n",
    "    '''\n",
    "    def update_model(self):\n",
    "        # Go through the episode, step-by-step and make policy updates (note we sometime use j for the individual steps)\n",
    "        for i in range(self.steps):   \n",
    "            # Get one-hot encodings of action-space, while log state gets converted to log space\n",
    "            # TODO (for Fahad): determine if log space is still effective here\n",
    "            log_state = self.__get_state_encoding(self.episode.states[i])\n",
    "            action_onehot = np_utils.to_categorical(self.episode.actions[i], num_classes=self.dim_action[0]) \n",
    "\n",
    "            # The return, G_t, after this timestep; this is the target for the PolicyEstimator\n",
    "            G_t = sum(self.gamma**n * r for n, r in enumerate(self.episode.rewards[i:]))\n",
    "\n",
    "            # Update our policy estimator\n",
    "            self.train_fn([log_state, action_onehot, np.array(G_t)]) # call the custom optimiser which takes a gradient step\n",
    "        \n",
    "    '''\n",
    "    Reset episode data\n",
    "    '''\n",
    "    def reset(self):\n",
    "        self.steps = 0\n",
    "        self.episode = EpisodeSteps(states=[], actions=[], rewards=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/kernelized.py:206: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-842eb786ad3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpisodeStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Set up policy search agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mps_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicySearchAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c1737193a987>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, alpha, gamma, rbf_type, out_dim_rbf)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Build and train neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__build_train_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     '''\n",
      "\u001b[0;32m<ipython-input-6-c1737193a987>\u001b[0m in \u001b[0;36m__build_train_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m         self.train_fn = K.function(inputs=[self.model.input, action_onehot_placeholder, target],\n\u001b[1;32m     76\u001b[0m                                    \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                                    updates=updates) # return a function which, when called, takes a gradient step\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   3952\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3953\u001b[0m   return GraphExecutionFunction(\n\u001b[0;32m-> 3954\u001b[0;31m       inputs, outputs, updates=updates, name=name, **kwargs)\n\u001b[0m\u001b[1;32m   3955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, name, **session_kwargs)\u001b[0m\n\u001b[1;32m   3677\u001b[0m     \u001b[0;31m# dependencies in call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3678\u001b[0m     \u001b[0;31m# Index 0 = total loss or model output for `predict`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3679\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3680\u001b[0m       \u001b[0mupdates_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3681\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "env = virl.Epidemic(stochastic=False, noisy=False)\n",
    "\"\"\"\n",
    "Args:\n",
    "    stochastic (bool): Is the infection rate sampled from some distribution at the beginning of each episode (default: False)?\n",
    "    noisy (bool): Is the state a noisy estimate of the true state (default: False)?\n",
    "    problem_id (int): Deterministic parameterization of the epidemic (default: 0).\n",
    "\"\"\"\n",
    "\n",
    "num_episodes = 10\n",
    "episode_counter = 0\n",
    "\n",
    "stats = EpisodeStats(episode_lengths=[], episode_rewards=[])\n",
    "# Set up policy search agent\n",
    "ps_agent = PolicySearchAgent(env, alpha=0.1, gamma=0.1)\n",
    "    \n",
    "for i in range(num_episodes):\n",
    "    # Set up initial state\n",
    "    \n",
    "    s = env.reset()\n",
    "    states.append(s)\n",
    "    done = False\n",
    "\n",
    "    # Run simulation\n",
    "    while not done:\n",
    "        # Get policy to give set of action probs based on latest state and choose largest prob\n",
    "        action_probs = ps_agent.predict(states[-1])\n",
    "        choice = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "\n",
    "        # Make a step in environemnt\n",
    "        s, r, done, i = env.step(action=choice) \n",
    "        \n",
    "        # Update policy\n",
    "        ps_agent.increment(s, choice, r)\n",
    "\n",
    "    # Debug info\n",
    "    print(\"Episode: \" + str(episode_counter))\n",
    "    print(\"Action Probabilities: \" + str(action_probs))\n",
    "    #plot_states_and_rewards(states, rewards)\n",
    "    plot_graphs(ps_agent.episode)\n",
    "    \n",
    "    episode_counter += 1\n",
    "    \n",
    "    # Save episode's total reward\n",
    "    stats.episode_lengths.append(ps_agent.steps)\n",
    "    stats.episode_rewards.append(sum(ps_agent.episode.rewards))\n",
    "    \n",
    "    # Update model to learn from the rewards of current episode\n",
    "    ps_agent.update_model()\n",
    "    \n",
    "    # Reset ps_agent's episode data\n",
    "    ps_agent.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(stats)\n",
    "plot_episode_stats(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
